{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSport Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from astropy.io import ascii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML('<iframe src=https://services.datasport.com/2015/lauf/transviamala/alfaw.htm width=1000 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_probability_of_split(table, column):\n",
    "    nb_blank = 0\n",
    "    for line in table:\n",
    "        if len(line) > column and line[column] == ' ':\n",
    "            nb_blank += 1\n",
    "    return nb_blank / len(table)\n",
    "\n",
    "def split_separator(probabilities, separator):\n",
    "    max_prob = max(probabilities, key=lambda x: x['probability'])\n",
    "    index = max_prob['index']\n",
    "    separator = separator[:index] + ' ' + separator[index + 1:]\n",
    "    return separator\n",
    "\n",
    "def read_table(content):\n",
    "    header = content[0]\n",
    "    separator = content[1]\n",
    "    table = content[2:]\n",
    "    \n",
    "    blank = False\n",
    "    probabilities = []\n",
    "    \n",
    "    for column in range(0, len(header)):\n",
    "        if header[column] == ' ':\n",
    "            blank = True\n",
    "            new_prob = {'index': 0, 'probability': 0}\n",
    "            new_prob['index'] = column\n",
    "            new_prob['probability'] = get_probability_of_split(table, column)\n",
    "            probabilities.append(new_prob)\n",
    "            side = 'right'\n",
    "        elif blank and header[column] != ' ':\n",
    "            blank = False\n",
    "            separator = split_separator(probabilities, separator)\n",
    "            probabilities = []\n",
    "\n",
    "    final_content = [header, separator] + table\n",
    "\n",
    "    return ascii.read(final_content, format='fixed_width_two_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page = rq.get('https://services.datasport.com/2015/lauf/transviamala/alfac.htm')\n",
    "soup = bfs(page.text, 'html5lib')\n",
    "\n",
    "content = get_content(soup)\n",
    "\n",
    "read_table(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = rq.get('https://services.datasport.com/2009/diverse/trophy/alfac.htm')\n",
    "soup = bfs(page.text, 'html5lib')\n",
    "\n",
    "content = get_content(soup)\n",
    "\n",
    "# TODO fix ...\n",
    "#read_table(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scraping a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML('<iframe src=https://services.datasport.com/2015/lauf/transviamala width=1000 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_data_from_page(url):\n",
    "    data = []\n",
    "    page = rq.get(url)\n",
    "    print('status request: ' + str(page.status_code))\n",
    "    soup = bfs(page.text, 'html5lib')\n",
    "    \n",
    "    table_linksCap = soup.select('font > a[href*=ALF]')\n",
    "    # Elements in 1999 have Alf taf instead ALF\n",
    "    table_linksNor = soup.select('font > a[href*=Alf]')\n",
    "    \n",
    "    if table_linksCap > table_linksNor:\n",
    "        table_links =  table_linksCap\n",
    "    else:\n",
    "        table_links =  table_linksNor\n",
    "    \n",
    "    for idx, link in enumerate(table_links):\n",
    "        full_link = url + link['href']\n",
    "        \n",
    "        print(str(idx+1) + '/' + str(len(table_links)) + ' - Processing ' + full_link)\n",
    "        \n",
    "        alpha_page = rq.get(full_link)\n",
    "        alpha_bfs = bfs(alpha_page.text, 'html5lib')\n",
    "        df = create_clean_dataframe(get_raw_table(alpha_bfs).text, get_header(alpha_bfs).text)\n",
    "        data.append(df)\n",
    "        \n",
    "        time.sleep(random.uniform(0, 0.5))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = get_all_data_from_page('https://services.datasport.com/1999/lauf/sion/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get data from all run event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_events_df = pd.read_csv('Data/run_events.csv')\n",
    "run_events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_from_run_events(run_events, path='Data/ScrappingAcodeByRuns/'):\n",
    "\n",
    "    nbRaceProcessing = 0;\n",
    "    for run_event in run_events.itertuples():\n",
    "        print('Processing: Run \"' + run_event.name + '\" / Date ' + run_event.full_date + ' / Url ' + run_event.url)\n",
    "        print('Processing the url: ' + run_event.url)\n",
    "        data_run_events = get_all_data_from_page( run_event.url + '/')\n",
    "        \n",
    "        # Merge the data.\n",
    "        result_race_event = pd.concat(data_run_events)\n",
    "        print(len(result_race_event))\n",
    "            \n",
    "        # write on csv.\n",
    "        result_race_event.to_csv(path + run_event.name + '.csv')\n",
    "        \n",
    "        # test 10 first url.\n",
    "        nbRaceProcessing += 1\n",
    "        if nbRaceProcessing > 10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get_data_from_run_events(run_events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> URL qui pose probleme, Ã  regarder de plus pres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_run_event = get_all_data_from_page('http://services.datasport.com/2009/diverse/trophy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = get_all_data_from_page('http://services.datasport.com/1999/lauf/Greifenseelauf/')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
