{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSport Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from astropy.io import ascii\n",
    "import math\n",
    "import os\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML('<iframe src=https://services.datasport.com/2016/lauf/lamara/alfaa.htm width=1000 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_content(bfs_page):\n",
    "    fonts = bfs_page.select('pre > font')\n",
    "    text = ''\n",
    "    for font in fonts:\n",
    "        text += font.text\n",
    "    return text.split('\\n')\n",
    "\n",
    "def get_probability_of_split(table, column):\n",
    "    nb_blank = 0\n",
    "\n",
    "    for line in table:\n",
    "        if len(line) > column and line[column] == ' ':\n",
    "            nb_blank += 1\n",
    "\n",
    "    prob_column = nb_blank / len(table)\n",
    "\n",
    "    return prob_column\n",
    "\n",
    "def split_separator(probabilities, separator):\n",
    "    if len(probabilities) <= 0:\n",
    "        raise ValueError('No probability given')\n",
    "    max_prob = probabilities[0]\n",
    "    for probability in probabilities:\n",
    "        if probability['probability'] > max_prob['probability']:\n",
    "            max_prob = probability\n",
    "    index = max_prob['index']\n",
    "    separator = separator[:index] + ' ' + separator[index + 1:]\n",
    "    return separator\n",
    "\n",
    "def read_table(content):\n",
    "    header = content[0]\n",
    "    separator = content[1]\n",
    "    table = content[2:]\n",
    "    \n",
    "    blank = False\n",
    "    probabilities = []\n",
    "    \n",
    "    for column in range(0, len(header)):\n",
    "        if header[column] == ' ':\n",
    "            blank = True\n",
    "            new_prob = {'index': 0, 'probability': 0}\n",
    "            new_prob['index'] = column\n",
    "            new_prob['probability'] = get_probability_of_split(table, column)\n",
    "            probabilities.append(new_prob)\n",
    "        elif blank and header[column] != ' ':\n",
    "            blank = False\n",
    "            separator = split_separator(probabilities, separator)\n",
    "            probabilities = []\n",
    "\n",
    "    final_content = [header, separator] + table\n",
    "    \n",
    "    fill_values = [('-----', ''), ('---', ''), ('--', ''), ('-', ''), ('', '')]\n",
    "    \n",
    "    exclude_names = ['¦']\n",
    "\n",
    "    df = ascii.read(final_content, format='fixed_width_two_line', exclude_names=exclude_names, fill_values=fill_values).to_pandas()\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_acodes(df, soup, check_names=False):\n",
    "    runners = soup.findAll('span', attrs={'class': 'myds'})\n",
    "    \n",
    "    # Slow but we are sure that the acode match the runner\n",
    "    if check_names:\n",
    "        df['acode'] = ''\n",
    "        for runner in runners:\n",
    "            df.loc[df['nom'] == runner.text.strip(), 'acode'] = runner['acode']\n",
    "\n",
    "    # Very fast but strong assumption on the order of the dataframe and the acodes find\n",
    "    else:\n",
    "        acodes = [runner['acode'] for runner in runners]\n",
    "        # Need to add an extra acode for the last line\n",
    "        df['acode'] = acodes + ['']\n",
    "    return df\n",
    "\n",
    "def read_page(url, acodes=None):\n",
    "    page = rq.get(url)\n",
    "    soup = bfs(page.text, 'html5lib')\n",
    "    content = get_content(soup)\n",
    "    df = read_table(content)\n",
    "    if acodes and acodes == 'no-check':\n",
    "        df = add_acodes(df, soup, False)\n",
    "    elif acodes and acodes == 'check':\n",
    "        df = add_acodes(df, soup, True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "read_page('https://services.datasport.com/2016/lauf/lamara/alfaa.htm', 'no-check').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scraping a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML('<iframe src=https://services.datasport.com/2016/lauf/lamara/ width=1000 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_data_from_page(url, directory=False):\n",
    "    data = []\n",
    "    page = rq.get(url)\n",
    "    soup = bfs(page.text, 'html5lib')\n",
    "\n",
    "    table_links = soup.select('font > a[href*=ALF]')\n",
    "\n",
    "    for idx, link in enumerate(table_links):\n",
    "        full_link = url + link['href']\n",
    "        \n",
    "        print(str(idx+1) + '/' + str(len(table_links)) + ' - Processing ' + full_link)\n",
    "        df = read_page(full_link, 'no-check')        \n",
    "        data.append(df)\n",
    "        \n",
    "        if directory and os.access(directory, os.W_OK):\n",
    "            url_parsed = urlparse(full_link)\n",
    "            file = url_parsed.netloc + '_'.join(url_parsed.path.split('/')) + '.csv'\n",
    "            if directory[-1] == '/':\n",
    "                file = directory + file\n",
    "            else:\n",
    "                file = directory + '/' + file\n",
    "            df.to_csv(file)\n",
    "            print('Write file: ' + file)\n",
    "        \n",
    "        #time.sleep(random.uniform(0, 0.5))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_all_data_from_page('https://services.datasport.com/2016/lauf/lamara/', './Data/Lausanne_Marathon_2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_runners = 0\n",
    "for df in data:\n",
    "    nb_runners += len(df)\n",
    "\n",
    "nb_runners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Not used - Get data from all run event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_events_df = pd.read_csv('Data/run_events.csv')\n",
    "run_events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_from_run_events(run_events, path='Data/ScrappingAcodeByRuns/'):\n",
    "\n",
    "    nbRaceProcessing = 0;\n",
    "    for run_event in run_events.itertuples():\n",
    "        print('Processing: Run \"' + run_event.name + '\" / Date ' + run_event.full_date + ' / Url ' + run_event.url)\n",
    "        print('Processing the url: ' + run_event.url)\n",
    "        data_run_events = get_all_data_from_page( run_event.url + '/')\n",
    "        \n",
    "        # Merge the data.\n",
    "        result_race_event = pd.concat(data_run_events)\n",
    "        print(len(result_race_event))\n",
    "            \n",
    "        # write on csv.\n",
    "        result_race_event.to_csv(path + run_event.name + '.csv')\n",
    "        \n",
    "        # test 10 first url.\n",
    "        nbRaceProcessing += 1\n",
    "        if nbRaceProcessing > 10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get_data_from_run_events(run_events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> URL qui pose probleme, à regarder de plus pres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_run_event = get_all_data_from_page('http://services.datasport.com/2009/diverse/trophy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = get_all_data_from_page('http://services.datasport.com/1999/lauf/Greifenseelauf/')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
