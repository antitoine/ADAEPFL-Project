{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSport Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML('<iframe src=https://services.datasport.com/2015/lauf/transviamala/alfaw.htm width=1000 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_content(bfs_page):\n",
    "    fonts = bfs_page.select('pre > font')\n",
    "    text = ''\n",
    "    for font in fonts:\n",
    "        text += font.text\n",
    "    return text.split('\\n')\n",
    "\n",
    "def get_header(table):\n",
    "    return table[0]\n",
    "\n",
    "def get_raw_table(table):\n",
    "    return table[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_raw_table(raw, min_len=10, max_len=-1):\n",
    "    return [line for line in raw if len(line) > min_len and (max_len < 0 or len(line) < max_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_raw_table(raw, header=''):\n",
    "\n",
    "    lines = clean_raw_table(raw)\n",
    "\n",
    "    nb_char_max = len(max(lines, key=len))\n",
    "\n",
    "    values = [[] for _ in range(0, len(lines))]\n",
    "\n",
    "    column_char_idx = 0\n",
    "    column_names = []\n",
    "    column_idx = 0\n",
    "\n",
    "    for char_idx in range(0, nb_char_max):\n",
    "\n",
    "        # If all lines have a blank at the same character index,\n",
    "        # this is a separator and we need to split in two columns\n",
    "        nb_blank = 0\n",
    "        \n",
    "        # In order to don't add a full blank column,\n",
    "        # we need to check if a value is present a least in one line\n",
    "        has_value = False\n",
    "\n",
    "        for line_idx, line in enumerate(lines):\n",
    "\n",
    "            if len(line) > char_idx:\n",
    "                \n",
    "                if line[char_idx] == ' ':\n",
    "                    nb_blank += 1\n",
    "\n",
    "                # Init new column in line\n",
    "                if len(values[line_idx]) == column_idx:\n",
    "                    values[line_idx].append('')\n",
    "\n",
    "                # Add character if not blank or if there is already a value in the coumn\n",
    "                # (don't remove blank in a midle of a column)\n",
    "                if line[char_idx] != ' ' or len(values[line_idx][column_idx]) > 0:\n",
    "                    values[line_idx][column_idx] += line[char_idx]\n",
    "                    has_value = True\n",
    "\n",
    "            else:\n",
    "                nb_blank += 1\n",
    "\n",
    "        if nb_blank == len(lines) and has_value:\n",
    "            column_idx += 1            \n",
    "            column_name = header[column_char_idx:char_idx].strip()\n",
    "            column_names.append(column_name)\n",
    "            column_char_idx = char_idx\n",
    "\n",
    "    return values, column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_clean_dataframe(raw, header):\n",
    "    \n",
    "    data, columns = split_raw_table(raw, header)\n",
    "\n",
    "    clean_data = []\n",
    "    \n",
    "    for row in data:\n",
    "        clean_row = []\n",
    "        \n",
    "        # Remove the last column\n",
    "        for value in row[0:-1]:\n",
    "            \n",
    "            clean_value = value.strip()\n",
    "            if len(clean_value) > 0:\n",
    "                \n",
    "                # Remove the point present in last character of the value (like in rank)\n",
    "                if clean_value[len(clean_value)-1] == '.':\n",
    "                    clean_value = clean_value[0:-1]\n",
    "\n",
    "                # Remove parenthesis\n",
    "                if clean_value[0] == '(' and clean_value[len(clean_value)-1] == ')':\n",
    "                    clean_value = clean_value[1:-1]\n",
    "\n",
    "                # Set empty value when there is no real value\n",
    "                if clean_value[0] == '-' and clean_value[len(clean_value)-1] == '-':\n",
    "                    clean_value = ''\n",
    "\n",
    "            clean_row.append(clean_value)\n",
    "        clean_data.append(clean_row)\n",
    "\n",
    "    df = pd.DataFrame(data=clean_data, columns=columns[0:-1])\n",
    "    \n",
    "    \"\"\"\n",
    "        TODO:\n",
    "            - Set types\n",
    "            - Translate column names ?\n",
    "            - Fix: sometime the algo make unnecessary split, see : https://services.datasport.com/2015/lauf/transviamala/alfaw.htm\n",
    "    \"\"\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = rq.get('https://services.datasport.com/2015/lauf/transviamala/alfac.htm')\n",
    "soup = bfs(page.text, 'html5lib')\n",
    "\n",
    "content = get_content(soup)\n",
    "header = get_header(content)\n",
    "raw_table = get_raw_table(content)\n",
    "\n",
    "create_clean_dataframe(raw_table, header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scraping a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML('<iframe src=https://services.datasport.com/2015/lauf/transviamala width=1000 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_data_from_page(url):\n",
    "    data = []\n",
    "    page = rq.get(url)\n",
    "    print('status request: ' + str(page.status_code))\n",
    "    soup = bfs(page.text, 'html5lib')\n",
    "    \n",
    "    table_linksCap = soup.select('font > a[href*=ALF]')\n",
    "    # Elements in 1999 have Alf taf instead ALF\n",
    "    table_linksNor = soup.select('font > a[href*=Alf]')\n",
    "    \n",
    "    if table_linksCap > table_linksNor:\n",
    "        table_links =  table_linksCap\n",
    "    else:\n",
    "        table_links =  table_linksNor\n",
    "    \n",
    "    for idx, link in enumerate(table_links):\n",
    "        full_link = url + link['href']\n",
    "        \n",
    "        print(str(idx+1) + '/' + str(len(table_links)) + ' - Processing ' + full_link)\n",
    "        \n",
    "        alpha_page = rq.get(full_link)\n",
    "        alpha_bfs = bfs(alpha_page.text, 'html5lib')\n",
    "        df = create_clean_dataframe(get_raw_table(alpha_bfs).text, get_header(alpha_bfs).text)\n",
    "        data.append(df)\n",
    "        \n",
    "        time.sleep(random.uniform(0, 0.5))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_all_data_from_page('https://services.datasport.com/1999/lauf/sion/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in data:\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get data from all run event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_events_df = pd.read_csv('Data/run_events.csv')\n",
    "run_events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_from_run_events(run_events, path='Data/ScrappingAcodeByRuns/'):\n",
    "\n",
    "    nbRaceProcessing = 0;\n",
    "    for run_event in run_events.itertuples():\n",
    "        print('Processing: Run \"' + run_event.name + '\" / Date ' + run_event.full_date + ' / Url ' + run_event.url)\n",
    "        print('Processing the url: ' + run_event.url)\n",
    "        data_run_events = get_all_data_from_page( run_event.url + '/')\n",
    "        \n",
    "        # Merge the data.\n",
    "        result_race_event = pd.concat(data_run_events)\n",
    "        print(len(result_race_event))\n",
    "            \n",
    "        # write on csv.\n",
    "        result_race_event.to_csv(path + run_event.name + '.csv')\n",
    "        \n",
    "        # test 10 first url.\n",
    "        nbRaceProcessing += 1\n",
    "        if nbRaceProcessing > 10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_data_from_run_events(run_events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> URL qui pose probleme, à regarder de plus pres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_run_event = get_all_data_from_page('http://services.datasport.com/2009/diverse/trophy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_all_data_from_page('http://services.datasport.com/1999/lauf/Greifenseelauf/')\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
